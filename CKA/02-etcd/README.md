# üöÄ Learn Etcd

- [1) Vms Requisitos](#1-vms-requisitos)
- [2) Vms Startup](#2-vms-startup)
- [3) ETCD SystemD](#3-etcd-systemd)
- [4) ETCD StaticPod](#4-etcd-staticpod)


## 1) Vms Requisitos

Para execu√ß√£o desse laborat√≥rio √© necess√°rio ter o **KVM** instalado no host, pois ele ir√° emular todos as vms necess√°rias.

#### Imagens Cloud

√â necess√°rio baixar a imagem no formato **qcow2** que j√° vem adaptada para rodar com cloud-init. [Download Oficial](https://cloud-images.ubuntu.com/).

#### Diret√≥rio das Imagens

Esse script assumi que as imagens devam estar localizadas em **/var/lib/libvirt/images/**.

#### Chave SSH

√â necess√°rio ter um par de chaves SSH do tipo **id_ed25519** (privada e p√∫blica) moderno e seguro, baseado no algoritmo Ed25519 para autentica√ß√£o sem senha nos servidores.

```bash
ssh-keygen -t ed25519 -C "seu_email@exemplo.com"
```

#### Adicionar Entradas no seu /etc/hosts

Isso ir√° ajudar muito na conex√£o SSH

```bash
# Estudos Cka
#==============================
10.100.100.10       vip
10.100.100.11       master01
10.100.100.12       master02
10.100.100.13       master03
10.100.100.20       worker01
```

#### Password Default todas as Vms

Todas as vms tem uma senha default j√° definida no cloud-init **123456**.

## 2) Vms Startup

[Clone o Reposit√≥rio](https://github.com/Paulo-Rogerio/kubernetes-certifications.git), e navegue no diret√≥rio **01-kvm**. Nesse diret√≥rio, voc√™ ir√° encontrar um arquivo **hosts.txt**, ele √© quem ir√° definir a quantidade de hosts que ter√° seu laborat√≥rio.

```bash
#===========================================================================
# Estudos K8S
#===========================================================================
# Vm Name  |  Ram  | vCPU  |     Ip        |   Imagem Cloud Init
#===========================================================================
# master01   2048     3      10.100.100.11   jammy-server-cloudimg-amd64.img
# master02   2048     3      10.100.100.12   jammy-server-cloudimg-amd64.img
# worker01   2048     3      10.100.100.20   jammy-server-cloudimg-amd64.img
#===========================================================================
# Estudos ETCD
#===========================================================================
master01     2048     3      10.100.100.11    jammy-server-cloudimg-amd64.img
master02     2048     3      10.100.100.12    jammy-server-cloudimg-amd64.img
master03     2048     3      10.100.100.13    jammy-server-cloudimg-amd64.img
```

O instalador j√° garante que essa network ser√° criada, ent√£o **N√ÇO** altere o range **10.100.100.**. Essa rede √© criada no modo **NAT**, para evitar qualquer tipo de conflito.

#### Iniciando Vms

Esse utilit√°rio j√° cria todo os requisitos necess√°rios.

```bash
sh deploy.sh
```

Conecte-se na Vm rec√©m criada.

```bash
ssh root@master01
```

#### Removendo Vms

Remova as Vms ap√≥s o t√©rmino do laborat√≥rio.

```bash
sh remove.sh
```

## 3) ETCD SystemD

Para fins did√°ticos temos 2 op√ß√µes para rodar o **etcd**: Como servi√ßo **SystemD**, ou como **Static Pod**. Para entender como funciona o servi√ßo foi criado essas 2 implementa√ß√µes.

Ap√≥s deployado as Vms conecte-se em (**master01 e master02**).

Isso ir√° deployar o **etcd** externo , mantido pelo S.O e gerido pelo **systemd**.

```bash
ssh root@master01
cd kubernetes-certifications/CKA/02-etcd/etcd-systemd/
bash deploy-master01.sh
systemctl status etcd
```

Conecte-se na master02 e execute procedimento semelhante

```bash
ssh root@master02
cd kubernetes-certifications/CKA/02-etcd/etcd-systemd/
bash deploy-master02.sh
systemctl status etcd
```

#### OBS.: Subimos 3 Vms , mas o etcd foi instalado em apenas 2 membros. Isso foi proposital, para aprendermos manipular o ETCD.

A manipula√ß√£o dos dados no ETCD pode acontecer em qualquer node.

Os scripts para manipula√ß√£o dos **dados** ficam em: */root/kubernetes-certifications/CKA/02-etcd/etcd-manager/data*

```bash
cd /root/kubernetes-certifications/CKA/02-etcd/etcd-manager/data
bash 02-list-members.sh

+------------------+---------+----------+-----------------------+----------------------------+------------+
|        ID        | STATUS  |   NAME   |      PEER ADDRS       |        CLIENT ADDRS        | IS LEARNER |
+------------------+---------+----------+-----------------------+----------------------------+------------+
| 3713bd59c3918478 | started | master01 | https://master01:2380 | https://10.100.100.11:2379 |      false |
| 43450b1d21f3b96a | started | master02 | https://master02:2380 | https://10.100.100.12:2379 |      false |
+------------------+---------+----------+-----------------------+----------------------------+------------+
```

Os scripts para manipula√ß√£o do **cluster** ficam em: */root/kubernetes-certifications/CKA/02-etcd/etcd-manager/cluster/systemd*

Essa implementa√ß√£o baseada em **SystemD** √© apenas para clarificar como se comporta o servi√ßo. N√£o vou detalhar procedimentos aqui, visto que para **CKA** a implementa√ß√£o cobrada e baseada em **POD**

## 4) ETCD StaticPod


Ap√≥s deployado as Vms conecte-se em (**master01 e master02**). Para esse laborat√≥rio vamos deployar o **etcd** interno, mantido pelo e gerido pelo **kubernetes** via **StaticPod**.


Conecte-se na master01...

```bash
ssh root@master01
cd kubernetes-certifications/CKA/02-etcd/etcd-staticPod/
bash deploy.sh
crictl ps -a | grep etcd
crictl logs 992e3500eddf6
systemctl status kubelet
```

#### OBS.: O mesmo script atende ambas implementa√ß√µes

Conecte-se na master02 e execute procedimento semelhante

```bash
ssh root@master02
cd kubernetes-certifications/CKA/02-etcd/etcd-staticPod/
bash deploy.sh
crictl ps -a | grep etcd
crictl logs 992e3500eddf6
systemctl status kubelet
```
